import os
import time
import streamlit as st
from dotenv import load_dotenv
from langchain_cohere import CohereEmbeddings
from langchain_chroma import Chroma
from langchain.chat_models import init_chat_model

load_dotenv()

if not os.getenv("COHERE_API_KEY"):
    st.error("‚ö†Ô∏è Falta la variable de entorno COHERE_API_KEY.")
    st.stop()

embeddings = CohereEmbeddings(
    model="embed-multilingual-v3.0",
    cohere_api_key=os.getenv("COHERE_API_KEY"),
    user_agent="lekta-rag/0.1"
)
vector_store = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db"
)
llm = init_chat_model("meta-llama/llama-4-scout-17b-16e-instruct", model_provider="groq")

SYSTEM_PROMPT = (
    "Eres un asistente experto en responder preguntas usando solo la informaci√≥n proporcionada.\n"
    "Tu misi√≥n es maximizar la utilidad al usuario, sin desviarte jam√°s del contexto.\n\n"
    "OBJETIVOS:\n"
    "1. Responder con precisi√≥n y brevedad (‚â§ 40 palabras).\n"
    "2. Ayudar al usuario al m√°ximo con la informaci√≥n disponible.\n\n"
    "RESTRICCIONES:\n"
    "- No menciones la fuente, el contexto ni uses expresiones tipo ‚Äúseg√∫n‚Ä¶‚Äù, ‚Äúdocumentaci√≥n‚Äù.\n"
    "- No especules, conjetures ni inventes datos.\n"
    "- No uses saludos, despedidas ni frases de cortes√≠a.\n"
    "- Si no hay informaci√≥n suficiente, responde EXACTAMENTE:\n"
    "  ‚ÄúNo hay informaci√≥n disponible para responder a esta pregunta.‚Äù\n\n"
    "FORMATO:\n"
    "- Texto plano, m√°ximo 40 palabras.\n"
    "- Si aportas listas o vi√±etas, que sean muy breves (‚â§ 3 √≠tems).\n\n"
    "Pregunta: {question}\n"
    "Informaci√≥n: {context}\n"
    "Respuesta:"
)

with st.form("rag_form"):
    question = st.text_input("Pregunta")
    k = st.slider("Fragmentos a recuperar", 1, 10, 4)
    submitted = st.form_submit_button("Consultar")

if submitted and question.strip():
    with st.spinner("üîé Buscando respuesta..."):
        start = time.time()
        docs = vector_store.similarity_search(question, k=k)

        if not docs:
            st.warning("‚ö†Ô∏è No se recuperaron fragmentos.")
        else:
            max_chars = 3000
            context = ""
            for d in docs:
                if len(context) + len(d.page_content) <= max_chars:
                    context += d.page_content + "\n\n"
                else:
                    break

            prompt = SYSTEM_PROMPT.format(question=question, context=context)

            try:
                answer = llm.invoke(prompt).content
                st.success(answer)
            except Exception as e:
                st.error(f"Error al invocar el modelo: {str(e)}")
                st.stop()

            st.caption(f"‚è±Ô∏è Tiempo: {round(time.time() - start, 3)}s")

            with st.expander("üß© Fragmentos usados"):
                for i, d in enumerate(docs, 1):
                    st.markdown(f"**Fragmento {i}:**\n\n{d.page_content}")
